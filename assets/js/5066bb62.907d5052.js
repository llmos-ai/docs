"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[400],{1051:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var n=t(4848),o=t(8453);const i={sidebar_position:2,title:"System Storage"},a=void 0,r={id:"user_guide/storage/system-storage",title:"System Storage",description:"LLMOS provides two built-in storage service:",source:"@site/docs/user_guide/storage/system-storage.md",sourceDirName:"user_guide/storage",slug:"/user_guide/storage/system-storage",permalink:"/docs/user_guide/storage/system-storage",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/user_guide/storage/system-storage.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"System Storage"},sidebar:"tutorialSidebar",previous:{title:"Volumes",permalink:"/docs/user_guide/storage/volumes"},next:{title:"Advanced",permalink:"/docs/category/advanced"}},l={},d=[{value:"Ceph Prerequisites",id:"ceph-prerequisites",level:2},{value:"Enabling Ceph Storage",id:"enabling-ceph-storage",level:2},{value:"Ceph Toolbox",id:"ceph-toolbox",level:3},{value:"Disable Ceph Storage",id:"disable-ceph-storage",level:2},{value:"Delete the Data on Hosts",id:"delete-the-data-on-hosts",level:3},{value:"Zapping Devices",id:"zapping-devices",level:4}];function h(e){const s={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.p,{children:"LLMOS provides two built-in storage service:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.a,{href:"https://github.com/rancher/local-path-provisioner",children:"Local Path Storage"}),": This is a simple storage service that stores data on the host machine and is suitable for testing or single node purposes(No HA and data fault-tolerance)."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.a,{href:"https://rook.io/docs/rook/latest-release/Getting-Started/intro",children:"Ceph Cluster Storage"}),": This is a distributed storage service that provides high availability and data fault-tolerance. It provides block and object storage and is designed for medium to large scale production clusters."]}),"\n"]}),"\n",(0,n.jsx)(s.admonition,{type:"info",children:(0,n.jsxs)(s.p,{children:["Ceph storage is not enabled by default. Please enable it before using it. Also make sure that your cluster has enough resources and meets the ",(0,n.jsx)(s.a,{href:"#ceph-prerequisites",children:"prerequisites"})," below."]})}),"\n",(0,n.jsx)(s.h2,{id:"ceph-prerequisites",children:"Ceph Prerequisites"}),"\n",(0,n.jsx)(s.p,{children:"To configure the Ceph storage cluster, at least one of these local storage types is required:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Raw devices (no partitions or formatted filesystems)"}),"\n",(0,n.jsx)(s.li,{children:"Raw partitions (no formatted filesystem)"}),"\n",(0,n.jsx)(s.li,{children:"LVM Logical Volumes (no formatted filesystem)"}),"\n",(0,n.jsx)(s.li,{children:"Persistent Volumes available from a storage class in block mode"}),"\n"]}),"\n",(0,n.jsx)(s.p,{children:"To confirm whether the partitions or devices are formatted with filesystems with the following command:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-shell",children:"$ lsblk -f\nNAME                  FSTYPE      LABEL UUID                                   MOUNTPOINT\nvda\n\u2514\u2500vda1                LVM2_member       >eSO50t-GkUV-YKTH-WsGq-hNJY-eKNf-3i07IB\n  \u251c\u2500ubuntu--vg-root   ext4              c2366f76-6e21-4f10-a8f3-6776212e2fe4   /\n  \u2514\u2500ubuntu--vg-swap_1 swap              9492a3dc-ad75-47cd-9596-678e8cf17ff9   [SWAP]\nvdb\n"})}),"\n",(0,n.jsxs)(s.p,{children:["If the ",(0,n.jsx)(s.code,{children:"FSTYPE"})," field is not empty, there is a filesystem on top of the corresponding device. In this example, ",(0,n.jsx)(s.code,{children:"vdb"})," is available to use, while ",(0,n.jsx)(s.code,{children:"vda"})," and its partitions have a filesystem and are not available."]}),"\n",(0,n.jsxs)(s.p,{children:["For more information, please refer to the ",(0,n.jsx)(s.a,{href:"https://rook.github.io/docs/rook/latest-release/Getting-Started/Prerequisites/prerequisites#ceph-prerequisites",children:"Rook Ceph Prerequisites"}),"."]}),"\n",(0,n.jsx)(s.h2,{id:"enabling-ceph-storage",children:"Enabling Ceph Storage"}),"\n",(0,n.jsxs)(s.p,{children:["If the Ceph storage is not enabled, you should be able to see the enabling notification in the ",(0,n.jsx)(s.strong,{children:"Home"})," page. Click the ",(0,n.jsx)(s.code,{children:"Enable"})," link to go to the configuration page."]}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Enable Ceph Notification",src:t(4929).A+"",width:"3416",height:"796"})}),"\n",(0,n.jsxs)(s.p,{children:["Configure the Ceph cluster, block pools, filesystem and other parameters if you want. Then click ",(0,n.jsx)(s.code,{children:"Save"})," to enable the Ceph storage."]}),"\n",(0,n.jsx)(s.admonition,{type:"note",children:(0,n.jsxs)(s.p,{children:["Default value should be enough for most cases. Refer to the ",(0,n.jsx)(s.a,{href:"https://rook.github.io/docs/rook/latest-release/Helm-Charts/ceph-cluster-chart/#configuration",children:"Ceph Cluster Configuration"})," for more details."]})}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Enable Ceph Configs",src:t(1967).A+"",width:"3188",height:"1752"})}),"\n",(0,n.jsxs)(s.p,{children:["Wait for the Ceph cluster ",(0,n.jsx)(s.code,{children:"Phase"})," to be ",(0,n.jsx)(s.strong,{children:"Ready"}),". You can also check the Ceph Block Pool and Filesystem status in the related pages under the ",(0,n.jsx)(s.code,{children:"Storage"})," menu."]}),"\n",(0,n.jsxs)(s.admonition,{type:"note",children:[(0,n.jsxs)(s.p,{children:["If you can't see the Ceph cluster, please try to filter the namespace with ",(0,n.jsx)(s.code,{children:"All Namespaces"})," option in the top navigation bar."]}),(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Namespace Filter",src:t(8910).A+"",width:"3190",height:"410"})})]}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Ceph Status",src:t(226).A+"",width:"3112",height:"858"})}),"\n",(0,n.jsxs)(s.p,{children:["Now you can use the default ",(0,n.jsx)(s.code,{children:"llmos-ceph-block"})," or ",(0,n.jsx)(s.code,{children:"llmos-ceph-filesystem"})," StorageClass to create volumes associated with the workloads."]}),"\n",(0,n.jsx)(s.h3,{id:"ceph-toolbox",children:"Ceph Toolbox"}),"\n",(0,n.jsx)(s.p,{children:"To access the Ceph cluster, you can use the Ceph toolbox to execute Ceph commands."}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["Go to the ",(0,n.jsx)(s.strong,{children:"Storage > Ceph Clusters"})," page and click the ",(0,n.jsx)(s.code,{children:"Execute Toolbox"})," button of the ",(0,n.jsx)(s.strong,{children:"llmos-ceph"})," cluster.\n",(0,n.jsx)(s.img,{alt:"Ceph Toolbox",src:t(6927).A+"",width:"3104",height:"636"})]}),"\n",(0,n.jsxs)(s.li,{children:["This will open a new window with the Ceph toolbox pod running in the background. And all available tools in the toolbox are ready for your troubleshooting needs. For example, you can run:","\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-shell",children:"ceph status\nceph osd status\nceph df\nrados df\n"})}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"disable-ceph-storage",children:"Disable Ceph Storage"}),"\n",(0,n.jsxs)(s.admonition,{type:"warning",children:[(0,n.jsx)(s.p,{children:(0,n.jsx)(s.strong,{children:"DATA WILL BE PERMANENTLY DELETED AFTER DELETING THE Ceph Cluster."})}),(0,n.jsx)(s.p,{children:"Please make sure that you have a backup of your data before disabling the Ceph storage."})]}),"\n",(0,n.jsx)(s.p,{children:"Before disabling the Ceph storage, please make sure that all workloads using the Ceph storage are deleted and the volumes are detached first."}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Delete all workloads using the Ceph storage."}),"\n",(0,n.jsxs)(s.li,{children:["Go to the ",(0,n.jsx)(s.strong,{children:"Volumes"})," page and make sure that all volumes that use the Ceph storage class are removed correctly."]}),"\n",(0,n.jsxs)(s.li,{children:["Now go to the ",(0,n.jsx)(s.strong,{children:"Managed Addons"})," page and click the ",(0,n.jsx)(s.code,{children:"Disable"})," button of the ",(0,n.jsx)(s.strong,{children:"llmos-ceph-cluster"})," addon."]}),"\n",(0,n.jsxs)(s.li,{children:["Click ",(0,n.jsx)(s.code,{children:"Save"})," to disable the Ceph storage."]}),"\n"]}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"Disable Ceph",src:t(5827).A+"",width:"3104",height:"810"})}),"\n",(0,n.jsx)(s.h3,{id:"delete-the-data-on-hosts",children:"Delete the Data on Hosts"}),"\n",(0,n.jsx)(s.admonition,{type:"info",children:(0,n.jsxs)(s.p,{children:["The final cleanup step requires deleting files on each host in the cluster. All files under the dataDirHostPath ",(0,n.jsx)(s.code,{children:"/var/lib/llmos/rook-ceph"})," property specified in the cluster will need to be deleted. Otherwise, inconsistent state will remain when re-enabling the Ceph storage."]})}),"\n",(0,n.jsxs)(s.p,{children:["Connect to each machine and delete the namespace directory under dataDirHostPath, by default ",(0,n.jsx)(s.code,{children:"/var/lib/llmos/rook-ceph"}),"."]}),"\n",(0,n.jsx)(s.h4,{id:"zapping-devices",children:"Zapping Devices"}),"\n",(0,n.jsx)(s.p,{children:"Disks on nodes used by Ceph for OSDs can be reset to a usable state. Note that these scripts are not one-size-fits-all. Please use them with discretion to ensure you are not removing data unrelated to Ceph."}),"\n",(0,n.jsx)(s.p,{children:"A single disk can usually be cleared with some or all of the steps below."}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-shell",children:'DISK="/dev/sdX"\n\n# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)\nsgdisk --zap-all $DISK\n\n# Wipe a large portion of the beginning of the disk to remove more LVM metadata that may be present\ndd if=/dev/zero of="$DISK" bs=1M count=100 oflag=direct,dsync\n\n# SSDs may be better cleaned with blkdiscard instead of dd\nblkdiscard $DISK\n\n# Inform the OS of partition table changes\npartprobe $DISK\n'})}),"\n",(0,n.jsx)(s.p,{children:"Ceph can leave LVM and device mapper data on storage drives, preventing them from being redeployed. These steps can clean former Ceph drives for reuse. Note that this only needs to be run once on each node. If you have only one Ceph cluster and all Ceph disks are being wiped, run the following command."}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-shell",children:"# This command hangs on some systems: with caution, 'dmsetup remove_all --force' can be used\nls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %\n\n# ceph-volume setup can leave ceph-<UUID> directories in /dev and /dev/mapper (unnecessary clutter)\nrm -rf /dev/ceph-*\nrm -rf /dev/mapper/ceph--*\n"})}),"\n",(0,n.jsx)(s.p,{children:"If disks are still reported locked, rebooting the node often helps clear LVM-related holds on disks."}),"\n",(0,n.jsx)(s.p,{children:"If there are multiple Ceph clusters and some disks are not wiped yet, it is necessary to manually determine which disks map to which device mapper devices."}),"\n",(0,n.jsxs)(s.p,{children:["For more detailed information, please refer to the ",(0,n.jsx)(s.a,{href:"https://rook.github.io/docs/rook/latest-release/Getting-Started/ceph-teardown/?h=cleanup#troubleshooting",children:"Rook Ceph Cleanup"})," guide."]})]})}function c(e={}){const{wrapper:s}={...(0,o.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},5827:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/ceph-addon-disable-8e84adaf9b01a89d829d4b6e626da634.png"},226:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/ceph-cluster-ready-ec03d68b52cf70f142b6065f60de7050.png"},1967:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/ceph-enable-configs-7325f6e0998c349117cb96c030638697.png"},4929:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/ceph-enable-notification-60ef4223b20931db019140f797a70150.png"},8910:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/ceph-ns-filter-b248d17aeefcdbb59989d1e1519d5ddd.png"},6927:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/ceph-toolbox-8079b3015b66f8239eaeb349ac861076.png"},8453:(e,s,t)=>{t.d(s,{R:()=>a,x:()=>r});var n=t(6540);const o={},i=n.createContext(o);function a(e){const s=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),n.createElement(i.Provider,{value:s},e.children)}}}]);