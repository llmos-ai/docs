"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[361],{8321:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var r=n(4848),i=n(8453);const o={sidebar_position:1,title:"Overview"},s="LLMOS Overview",a={id:"index",title:"Overview",description:"LLMOS is an open-source, cloud-native infrastructure software tailored for managing AI applications and large language models(LLMs) on your AI workstation or GPU machines.",source:"@site/docs/index.md",sourceDirName:".",slug:"/",permalink:"/docs/",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Overview"},sidebar:"tutorialSidebar",next:{title:"Quickstart",permalink:"/docs/quickstart"}},l={},c=[{value:"LLMOS Architecture",id:"llmos-architecture",level:2},{value:"Key Features",id:"key-features",level:2}];function d(e){const t={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"llmos-overview",children:"LLMOS Overview"})}),"\n",(0,r.jsx)(t.p,{children:"LLMOS is an open-source, cloud-native infrastructure software tailored for managing AI applications and large language models(LLMs) on your AI workstation or GPU machines."}),"\n",(0,r.jsx)(t.h2,{id:"llmos-architecture",children:"LLMOS Architecture"}),"\n",(0,r.jsx)(t.p,{children:"The following diagram describes the high-level LLMOS architecture:"}),"\n",(0,r.jsx)(t.h2,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Easy to install:"})," Install directly on the x86_64 or ARM64 architecture, offering an out-of-the-box user experience."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Complete Infrastructure & LLM Lifecycle Management:"})," Provides a unified interface for both developers and non-developers to manage the LLM infrastructure, ML Cluster, models and workloads."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Easy to use:"})," Build models and AI applications in your own way, without needing to managing Kubernetes & infrastructure directly."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Perfect for edge & branch:"})," better resource optimization, simplify the deployment of models and workloads to edge and branch networks, but can also scale up horizontally to handle large workloads."]}),"\n"]})]})}function u(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>a});var r=n(6540);const i={},o=r.createContext(i);function s(e){const t=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:t},e.children)}}}]);