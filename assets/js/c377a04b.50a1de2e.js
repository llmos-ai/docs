"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[3361],{8321:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var r=n(4848),t=n(8453);const i={sidebar_position:1,title:"Overview"},o="LLMOS Overview",a={id:"index",title:"Overview",description:"LLMOS is an open-source cloud-native AI infrastructure software designed to simplify the management of AI applications and Large Language Models (LLMs)  on your AI workstation or GPU machines.",source:"@site/docs/index.md",sourceDirName:".",slug:"/",permalink:"/docs/",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Overview"},sidebar:"tutorialSidebar",next:{title:"Quickstart",permalink:"/docs/quickstart"}},l={},c=[{value:"LLMOS Architecture",id:"llmos-architecture",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Next Step",id:"next-step",level:2}];function d(e){const s={a:"a",admonition:"admonition",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"llmos-overview",children:"LLMOS Overview"})}),"\n",(0,r.jsx)(s.p,{children:"LLMOS is an open-source cloud-native AI infrastructure software designed to simplify the management of AI applications and Large Language Models (LLMs)  on your AI workstation or GPU machines.\nWith LLMOS, organizations can effortlessly deploy, scale, and operate machine learning workflows while reducing the complexity often associated with AI development and operations."}),"\n",(0,r.jsx)(s.h2,{id:"llmos-architecture",children:"LLMOS Architecture"}),"\n",(0,r.jsx)(s.p,{children:"The following diagram describes the high-level LLMOS architecture:"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"LLMOS Architecture",src:n(2818).A+""})}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Server Node"}),": The Server Node is a cloud-based or on-premises machine that hosts the LLMOS platform along with LLMOS-optimized Kubernetes."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Worker Node"}),": The Worker Node is primarily responsible for executing the user workloads."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LLMOS-Operator"}),": The LLMOS-Operator manages the lifecycle and system components of the LLMOS platform, including LLMOS API-server, LLMOS controllers, and additional system addons."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LLMOS-Controller"}),": The LLMOS-Controller is responsible for managing the lifecycle and resources like LLM models, notebooks, machine learning cluster, jobs and so on."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Redis"}),": A key-value store used for storing LLMOS's fault-tolerant configurations and API chats."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Workloads"}),": Workloads are computational tasks that run on the LLMOS infrastructure, utilizing requested resources (e.g., CPU, GPU, memory, and storage volumes)."]}),"\n"]}),"\n",(0,r.jsxs)(s.admonition,{type:"note",children:[(0,r.jsx)(s.mdxAdmonitionTitle,{}),(0,r.jsx)(s.p,{children:"Server nodes also function as worker nodes, but prioritize resources to the system components first."})]}),"\n",(0,r.jsx)(s.h2,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:["Easy ",(0,r.jsx)(s.a,{href:"./quickstart",children:"Installation"}),":"]})," Simple to install on both x86_64 and ARM64 architectures, delivering an out-of-the-box user experience."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_guide/ml_clusters",children:"Machine Learning Cluster"}),":"]})," Supports distributed computing with parallel processing capabilities and access to leading AI libraries, improving the performance of machine learning workflows\u2014especially for large-scale models and datasets."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:["Seamless ",(0,r.jsx)(s.a,{href:"/docs/user_guide/notebooks",children:"Notebook"})," Integration:"]})," Integrates with popular notebook environments such as ",(0,r.jsx)(s.strong,{children:"Jupyter"}),", ",(0,r.jsx)(s.strong,{children:"VSCode"}),", and ",(0,r.jsx)(s.strong,{children:"RStudio"}),", allowing data scientists and developers to work efficiently in familiar tools without complex setup."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"/docs/user_guide/modelservice",children:"ModelService"})," for LLM Serving:"]})," Easily serve LLMs using ModelService with ",(0,r.jsx)(s.strong,{children:"OpenAI-compatible APIs"}),"."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_guide/monitoring/enable-monitoring",children:"Monitoring & Alerts"}),":"]})," Makes it easy to track cluster and GPU metrics with ready-to-use Grafana dashboards, Prometheus rules, alerts, and more, using the Prometheus Operator."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:["Built-in ",(0,r.jsx)(s.a,{href:"./user_guide/storage/system-storage",children:"Distributed Storage"}),":"]})," Provides built-in distributed storage with high-performance, fault-tolerant features. Offers robust, scalable block and filesystem storage tailored to the demands of AI and LLM applications."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsxs)(s.strong,{children:[(0,r.jsx)(s.a,{href:"./user_and_auth/user",children:"User"})," & ",(0,r.jsx)(s.a,{href:"./user_and_auth/role-template",children:"RBAC Management"}),":"]})," Simplifies user management with role-based access control (RBAC) and role templates, ensuring secure and efficient resource allocation."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Optimized for Edge & Branch Deployments:"})," Supports private deployments with optimized resource usage for running models and workloads in edge and branch networks. It also allows for horizontal scaling to accommodate future business needs."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"AI Research & Development:"})," Simplifies LLM and AI infrastructure management, enabling researchers to focus on innovation rather than operational complexities."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Enterprise AI Solutions:"})," Streamline the deployment of AI applications with scalable infrastructure, making it easier to manage models, storage, and resources across multiple teams."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Data Science Workflows:"})," With notebook integration and powerful cluster computing, LLMOS is ideal for data scientists looking to run complex experiments at scale."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"AI-Driven Products:"})," From chatbots to automated content generation, LLMOS simplifies the process of deploying LLM-based products that can serve millions of users and scale up horizontally."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"next-step",children:"Next Step"}),"\n",(0,r.jsxs)(s.p,{children:["To get started with LLMOS, please refer to the ",(0,r.jsx)(s.a,{href:"./quickstart",children:"Quick Start"})," guide."]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},2818:(e,s,n)=>{n.d(s,{A:()=>r});const r=n.p+"assets/images/llmos-arch-c8b1ff51c1c65ccc0c5b543687aa2c28.svg"},8453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>a});var r=n(6540);const t={},i=r.createContext(t);function o(e){const s=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:s},e.children)}}}]);