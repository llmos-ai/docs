"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[324],{7355:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var t=s(4848),i=s(8453);const r={sidebar_position:2,title:"Machine Learning Clusters"},a=void 0,o={id:"user_guide/ml_clusters",title:"Machine Learning Clusters",description:"A Machine Learning (ML) Cluster provides a distributed computing environment for running machine learning workloads. Built on top of Ray, a unified framework for scaling AI and Python applications, it provides a distributed runtime, parallel processing, and a suite of AI libraries to accelerate your machine learning tasks.",source:"@site/docs/user_guide/ml_clusters.md",sourceDirName:"user_guide",slug:"/user_guide/ml_clusters",permalink:"/docs/user_guide/ml_clusters",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/user_guide/ml_clusters.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Machine Learning Clusters"},sidebar:"tutorialSidebar",previous:{title:"Nodes",permalink:"/docs/user_guide/nodes"},next:{title:"LLM Management",permalink:"/docs/category/llm-management"}},l={},c=[{value:"Create a Machine Learning Cluster",id:"create-a-machine-learning-cluster",level:2},{value:"Head Group Configs",id:"head-group-configs",level:3},{value:"Worker Group Configs",id:"worker-group-configs",level:3},{value:"Advanced Configs",id:"advanced-configs",level:3},{value:"User Guide",id:"user-guide",level:2},{value:"Setup Ray Client",id:"setup-ray-client",level:3},{value:"Submitting a Ray Job",id:"submitting-a-ray-job",level:3},{value:"More Examples",id:"more-examples",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:["A Machine Learning (ML) Cluster provides a distributed computing environment for running machine learning workloads. Built on top of ",(0,t.jsx)(n.a,{href:"https://docs.ray.io",children:"Ray"}),", a unified framework for scaling AI and Python applications, it provides a distributed runtime, parallel processing, and a suite of AI libraries to accelerate your machine learning tasks."]}),"\n",(0,t.jsx)(n.p,{children:"ML Clusters simplify the integration and deployment of Ray clusters with existing cluster tools, such as monitoring, logging, and GPU accelerators. This enables you to effortlessly manage both individual and end-to-end machine learning workflows, leveraging the following features:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Dashboard:"})," Monitor and debug ML clusters, applications, and tasks from a single interface."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified API:"})," Run ML workloads on independent or shared ML clusters with a consistent API."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalable Libraries:"})," Access Ray libraries for common ML tasks, including data preprocessing, distributed training, hyperparameter tuning, reinforcement learning, and model serving."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pythonic Distributed Computing:"})," Utilize distributed computing primitives to parallelize and scale Python applications."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ml-cluster",src:s(4019).A+"",width:"3838",height:"1860"})}),"\n",(0,t.jsx)(n.h2,{id:"create-a-machine-learning-cluster",children:"Create a Machine Learning Cluster"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["ML Clusters will only support KubeRay CRDs starts from ",(0,t.jsx)(n.code,{children:"v1"})," version."]})}),"\n",(0,t.jsxs)(n.p,{children:["To create a machine learning cluster, navigate to the ",(0,t.jsx)(n.strong,{children:"LLMOS Management > Machine Learning Clusters"})," page and click ",(0,t.jsx)(n.strong,{children:"Create"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"head-group-configs",children:"Head Group Configs"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Specify the cluster ",(0,t.jsx)(n.strong,{children:"name"}),", and select the desired ",(0,t.jsx)(n.strong,{children:"namespace"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Config the CPU and memory resources for the head node(default: 2 vCPU, 4 GB)."}),"\n",(0,t.jsxs)(n.li,{children:["(Optional) Click ",(0,t.jsx)(n.strong,{children:"Show Advanced"})," to configure cluster advanced options:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Select ",(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/ray-core/fault_tolerance/gcs.html",children:"Enable GCS fault tolerance"})," to provides automatic cluster-level metadata recovery in the event of head node failures.(default: Enabled)"]}),"\n",(0,t.jsxs)(n.li,{children:["Select ",(0,t.jsx)(n.strong,{children:"Allow Scheduling on Head Node"})," to allow Ray tasks to run on the head node. (disabled by default, and you must increase the head node resources to support the task)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ml-cluster-create-default",src:s(7138).A+"",width:"3114",height:"1136"})}),"\n",(0,t.jsx)(n.h3,{id:"worker-group-configs",children:"Worker Group Configs"}),"\n",(0,t.jsx)(n.p,{children:"Worker group is a set of worker nodes connected to the head node with same resource configurations. The auto-scale capability can help to save resources by autoscale up and down nodes according to the resources requested by applications running on the cluster."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.strong,{children:"Replicas"})," to specify the default number of worker nodes on start."]}),"\n",(0,t.jsxs)(n.li,{children:["Configure the ",(0,t.jsx)(n.strong,{children:"Minimum Replicas"})," and ",(0,t.jsx)(n.strong,{children:"Maximum Replicas"})," to set the auto-scale range."]}),"\n",(0,t.jsxs)(n.li,{children:["Specify the ",(0,t.jsx)(n.strong,{children:"CPU"})," and ",(0,t.jsx)(n.strong,{children:"Memory"})," resources for each worker node(default: 4 vCPU, 8 GB).","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Add ",(0,t.jsx)(n.strong,{children:"GPU"})," resources to enable GPU acceleration(only support NVIDIA GPUs for now)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ml-cluster-worker-group",src:s(5978).A+"",width:"3104",height:"1196"})}),"\n",(0,t.jsx)(n.h3,{id:"advanced-configs",children:"Advanced Configs"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Config the ",(0,t.jsx)(n.strong,{children:"Ray version"})," if you want to use a specific Ray version."]}),"\n",(0,t.jsxs)(n.li,{children:["Auto-Scale Options:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Check ",(0,t.jsx)(n.strong,{children:"Enabling Autoscaling"})," to enable or disable auto-scaling for the worker group."]}),"\n",(0,t.jsxs)(n.li,{children:["Specify the ",(0,t.jsx)(n.strong,{children:"Idle Timeout"})," in seconds to auto-scale down the worker nodes to minimum replicas when there are no tasks running."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ml-cluster-advanced",src:s(2576).A+"",width:"3098",height:"754"})}),"\n",(0,t.jsx)(n.h2,{id:"user-guide",children:"User Guide"}),"\n",(0,t.jsx)(n.p,{children:"Once the cluster is created, you can use the Ray client to connect to the cluster and run your ML workloads at scale."}),"\n",(0,t.jsx)(n.h3,{id:"setup-ray-client",children:"Setup Ray Client"}),"\n",(0,t.jsx)(n.p,{children:"First, go to your Python project and install the Ray client via:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'pip install -U "ray"\n'})}),"\n",(0,t.jsxs)(n.p,{children:["If needed, see the ",(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/ray-overview/installation.html",children:"Ray installation guide"})," for more details on installing Ray."]}),"\n",(0,t.jsx)(n.h3,{id:"submitting-a-ray-job",children:"Submitting a Ray Job"}),"\n",(0,t.jsx)(n.p,{children:"To run a Ray Job, we also need to be able to send HTTP requests to a Ray Cluster. Let's start with a sample script that can be run locally. The following script uses Ray APIs to submit a task and print its return value:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# script.py\nimport ray\n\n@ray.remote\ndef hello_world():\n    return "hello world"\n\nray.init()\nprint(ray.get(hello_world.remote()))\n'})}),"\n",(0,t.jsx)(n.p,{children:"Run the script locally to verify that it works:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"python script.py\n2024-09-28 22:46:21,735\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265\nhello world\n"})}),"\n",(0,t.jsx)(n.p,{children:"Now let's submit a job to the remote ML cluster using the Python SDK."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ray-job.py\nfrom ray.job_submission import JobSubmissionClient, JobStatus\nimport time\n\nclient = JobSubmissionClient(\n   address="https://localhost:8005/api/v1/namespaces/default/services/http:ml-cluster1-head-svc:dashboard/proxy/", # Replace the url with your ML cluster\'s endpoint url\n   headers={"Authorization": "Bearer llmos-qcmqf:xxxxxxxxxxxxxxp4bgq"}, # Replace with your LLMOS API token\n   verify=False # Disable SSL verification or use a custom CA certificate\n)\n\njob_id = client.submit_job(\n   # Entrypoint shell command to execute\n   entrypoint="python script.py",\n   # Path to the local directory that contains the script.py file\n   runtime_env={"working_dir": "./"}\n)\nprint(job_id)\n\ndef wait_until_status(job_id, status_to_wait_for, timeout_seconds=5):\n   start = time.time()\n   while time.time() - start <= timeout_seconds:\n      status = client.get_job_status(job_id)\n      print(f"status: {status}")\n      if status in status_to_wait_for:\n         break\n      time.sleep(1)\n\n\nwait_until_status(job_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})\nlogs = client.get_job_logs(job_id)\nprint(logs)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"JobSubmissionClient"})," submits the job to the Ray cluster and returns the job ID. The job ID can be used to check the status of the job and retrieve its logs."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"python rayjob.py\n2024-09-28 22:57:49,615\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_aee80db9018e46d7.zip.\n2024-09-28 22:57:49,615\tINFO packaging.py:531 -- Creating a file package for local directory './'.\nraysubmit_5XExd25xnNqfYSYK\nstatus: PENDING\nstatus: RUNNING\nstatus: RUNNING\nstatus: SUCCEEDED\n2024-09-28 07:58:30,120\tINFO job_manager.py:527 -- Runtime env is setting up.\n2024-09-28 07:58:31,288\tINFO worker.py:1458 -- Using address 10.42.0.223:6379 set in the environment variable RAY_ADDRESS\n2024-09-28 07:58:31,289\tINFO worker.py:1598 -- Connecting to existing Ray cluster at address: 10.42.0.223:6379...\n2024-09-28 07:58:31,301\tINFO worker.py:1774 -- Connected to Ray cluster. View the dashboard at 10.42.0.223:8265\nhello world\n"})}),"\n",(0,t.jsx)(n.p,{children:"You can also check the job status and logs in the ML cluster dashboard."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ray-job-submission",src:s(4406).A+"",width:"3202",height:"1256"})}),"\n",(0,t.jsx)(n.h3,{id:"more-examples",children:"More Examples"}),"\n",(0,t.jsx)(n.p,{children:"In above, we have shown how to submit a simple job to a remote ML cluster. You can also run more advanced End-to-End ML workflows utilizing the Ray AI libraries. Here are some examples for reference:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html",children:"[Example] Data Loading and Preprocessing"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/train/examples/transformers/huggingface_text_classification.html",children:"[Example] Text classification with Ray"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/train/examples/pytorch/torch_detection.html",children:"[Example] Object detection with Ray"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/train/examples/xgboost/xgboost_example.html",children:"[Example] Machine learning on tabular data"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/ray-core/examples/automl_for_time_series.html",children:"[Example] AutoML for Time Series with Ray"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},2576:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ml-cluster-advanced-8ccb1bff0b6fd96d17556b41e8f0fe29.png"},7138:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ml-cluster-create-default-8ecea54df1974d0296c9515d720cc505.png"},5978:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ml-cluster-worker-group-44b17454d83cd8e6009c5b7dc8a909ae.png"},4019:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ml-cluster-ef15040621e744d531627ef66b326d31.png"},4406:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ray-job-submission-f3bb12fc6c7539763b67867ddebabb71.png"},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);