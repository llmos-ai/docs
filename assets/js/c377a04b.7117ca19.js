"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[361],{8321:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var t=r(4848),s=r(8453);const o={sidebar_position:1,title:"Overview"},i="LLMOS Overview",l={id:"index",title:"Overview",description:"LLMOS is an open-source, cloud-native infrastructure software tailored for managing AI applications and large language models(LLMs) on your AI workstation or GPU machines.",source:"@site/docs/index.md",sourceDirName:".",slug:"/",permalink:"/docs/",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Overview"},sidebar:"tutorialSidebar",next:{title:"Quickstart",permalink:"/docs/quickstart"}},a={},d=[{value:"LLMOS Architecture",id:"llmos-architecture",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Next Step",id:"next-step",level:2}];function c(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"llmos-overview",children:"LLMOS Overview"})}),"\n",(0,t.jsx)(n.p,{children:"LLMOS is an open-source, cloud-native infrastructure software tailored for managing AI applications and large language models(LLMs) on your AI workstation or GPU machines."}),"\n",(0,t.jsx)(n.h2,{id:"llmos-architecture",children:"LLMOS Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The following diagram describes the high-level LLMOS architecture:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"LLMOS Architecture",src:r(2818).A+""})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Server Node"}),": The Server Node is a cloud-based or on-premises machine that hosts the LLMOS platform along with LLMOS-optimized Kubernetes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Worker Node"}),": The Worker Node is primarily responsible for executing the user workloads."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLMOS-Operator"}),": The LLMOS-Operator manages the lifecycle and system components of the LLMOS platform, including LLMOS API-server, LLMOS controllers, and additional system addons."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLMOS-Controller"}),": The LLMOS-Controller is responsible for managing the lifecycle and resources like LLM models, notebooks, machine learning cluster, jobs and so on."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Redis"}),": A key-value store used for storing LLMOS's fault-tolerant configurations and API chats."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Workloads"}),": Workloads are computational tasks that run on the LLMOS infrastructure, utilizing requested resources (e.g., CPU, GPU, memory, and storage volumes)."]}),"\n"]}),"\n",(0,t.jsxs)(n.admonition,{type:"note",children:[(0,t.jsx)(n.mdxAdmonitionTitle,{}),(0,t.jsx)(n.p,{children:"Server nodes also function as worker nodes, but prioritize resources to the system components first."})]}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Easy to Install:"})," Install directly on the x86_64 or ARM64 architecture, offering an out-of-the-box user experience."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complete Infrastructure & LLM Lifecycle Management:"})," Provides a unified interface for both developers and non-developers to manage the LLM infrastructure, ML Cluster, models and workloads."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Easy to Use:"})," Build models and AI applications in your own way, without needing to managing Kubernetes & infrastructure directly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perfect for Edge & Branch:"})," Better resource optimization, simplify the deployment of models and workloads to edge and branch networks, but can also scale up horizontally to handle large workloads."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-step",children:"Next Step"}),"\n",(0,t.jsxs)(n.p,{children:["To get started with LLMOS, please refer to the ",(0,t.jsx)(n.a,{href:"./quickstart",children:"Quick Start"})," guide."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},2818:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/llmos-arch-4e14b3e28d69184dcf3b7454002d76f5.svg"},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var t=r(6540);const s={},o=t.createContext(s);function i(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);