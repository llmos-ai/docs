"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[613],{6063:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=s(4848),t=s(8453);const r={sidebar_position:4,title:"Model Serving"},o=void 0,l={id:"user_guide/llm_management/serve",title:"Model Serving",description:"The LLMOS platform provides a streamlined way to serve machine learning models using the ModelService resource. This resource offers a user-friendly interface to configure and manage model serving, leveraging the powerful vLLM serving engine. By specifying parameters like the model name, Hugging Face configurations, resource requirements, and more, users can easily set up and deploy models efficiently and at scale.",source:"@site/docs/user_guide/llm_management/serve.md",sourceDirName:"user_guide/llm_management",slug:"/user_guide/llm_management/serve",permalink:"/docs/user_guide/llm_management/serve",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/user_guide/llm_management/serve.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,title:"Model Serving"},sidebar:"tutorialSidebar",previous:{title:"Notebooks",permalink:"/docs/user_guide/llm_management/notebooks"},next:{title:"Storage",permalink:"/docs/category/storage"}},d={},c=[{value:"Creating a Model Service",id:"creating-a-model-service",level:2},{value:"General Configuration",id:"general-configuration",level:3},{value:"Resource Configuration",id:"resource-configuration",level:3},{value:"Volumes",id:"volumes",level:3},{value:"Node Scheduling",id:"node-scheduling",level:3},{value:"Accessing Model Service APIs",id:"accessing-model-service-apis",level:2},{value:"API Endpoints",id:"api-endpoints",level:3},{value:"API Usage Examples",id:"api-usage-examples",level:3},{value:"cURL Example",id:"curl-example",level:4},{value:"Python Example",id:"python-example",level:4},{value:"Notebooks Interaction",id:"notebooks-interaction",level:4},{value:"Adding a HuggingFace Token",id:"adding-a-huggingface-token",level:2}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["The LLMOS platform provides a streamlined way to serve machine learning models using the ",(0,i.jsx)(n.code,{children:"ModelService"})," resource. This resource offers a user-friendly interface to configure and manage model serving, leveraging the powerful ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"vLLM"})," serving engine. By specifying parameters like the model name, Hugging Face configurations, resource requirements, and more, users can easily set up and deploy models efficiently and at scale."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-list",src:s(7151).A+"",width:"3034",height:"792"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-a-model-service",children:"Creating a Model Service"}),"\n",(0,i.jsxs)(n.p,{children:["You can create one or more model services from the ",(0,i.jsx)(n.strong,{children:"LLM Management > Model Services"})," page."]}),"\n",(0,i.jsx)(n.h3,{id:"general-configuration",children:"General Configuration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Specify the name and namespace for your model service."}),"\n",(0,i.jsxs)(n.li,{children:["Enter the model name, either from ",(0,i.jsx)(n.a,{href:"https://huggingface.co/models",children:"Hugging Face"})," (e.g., ",(0,i.jsx)(n.code,{children:"Qwen/Qwen2.5-0.5B-Instruct"}),") or a persisted model volume path (e.g., ",(0,i.jsx)(n.code,{children:"/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["(Optional) Add any ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/models/engine_args.html",children:"additional engine arguments"}),", such as ",(0,i.jsx)(n.code,{children:"--dtype"})," or ",(0,i.jsx)(n.code,{children:"--max-model-len"}),", in the ",(0,i.jsx)(n.strong,{children:"Arguments"})," field."]}),"\n",(0,i.jsxs)(n.li,{children:["(Optional) Add Hugging Face Configuration:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["If the model is only authorized for downloading, select a ",(0,i.jsx)(n.a,{href:"#adding-a-huggingface-token",children:"secret credential"})," that contains the Hugging Face access token."]}),"\n",(0,i.jsxs)(n.li,{children:["Specify a custom Hugging Face Mirror URL if necessary (e.g., ",(0,i.jsx)(n.code,{children:"https://hf-mirror.com/"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["(Optional) Add additional ",(0,i.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/serving/env_vars.html",children:"environment variables"})," for the model service in the ",(0,i.jsx)(n.strong,{children:"Environment Variables"})," field."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-create-general",src:s(1607).A+"",width:"3120",height:"1732"})}),"\n",(0,i.jsx)(n.h3,{id:"resource-configuration",children:"Resource Configuration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CPU and Memory"}),": Set CPU and memory resources for the model service.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["You may refer to ",(0,i.jsx)(n.a,{href:"https://github.com/ray-project/llm-numbers",children:"LLM numbers"})," for getting a better understanding of the resources consumed by the model."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU Resources"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Configure the desired ",(0,i.jsx)(n.strong,{children:"GPU"})," and ",(0,i.jsx)(n.strong,{children:"Runtime Class"}),"(default to ",(0,i.jsx)(n.strong,{children:"nvidia"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["A minimum of ",(0,i.jsx)(n.strong,{children:"1 GPU"})," is required for the model service with the default ",(0,i.jsx)(n.code,{children:"vllm-openai"})," image."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-resources",src:s(1072).A+"",width:"3128",height:"1174"})}),"\n",(0,i.jsx)(n.h3,{id:"volumes",children:"Volumes"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Persistent Volume"}),": A default persistent volume is mounted to ",(0,i.jsx)(n.code,{children:"/root/.cache/huggingface/hub"})," to store downloaded model files.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["(Optional)You can add an existing volume that contains the model files to skip the download process using the ",(0,i.jsx)(n.strong,{children:"Add Volume"})," button."]}),"\n",(0,i.jsxs)(n.li,{children:["(Optional)You can add a shared volume(with StorageClass support ",(0,i.jsx)(n.strong,{children:"ReadWriteMany"})," mode) to share model files across multiple model services."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Shared Memory Allocation"}),": Mount an ",(0,i.jsx)(n.code,{children:"emptyDir"})," volume to ",(0,i.jsx)(n.code,{children:"/dev/shm"})," with ",(0,i.jsx)(n.strong,{children:"Medium"})," set to ",(0,i.jsx)(n.strong,{children:"Memory"}),". This creates a temporary in-memory filesystem, ideal for PyTorch tensor parallel inference, which uses shared memory between processes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"If not enabled, the model service will use the default shared memory(shm) size of 64 MiB."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"modelservice-create-volumes",src:s(5782).A+"",width:"3164",height:"1630"})}),"\n",(0,i.jsx)(n.h3,{id:"node-scheduling",children:"Node Scheduling"}),"\n",(0,i.jsx)(n.p,{children:"You can specify node constraints for scheduling your model service using node labels, or leave it as default to run on any available node."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-node-scheduling",src:s(861).A+"",width:"3110",height:"1098"})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["For more details, refer to the ",(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity",children:"Kubernetes Node Affinity Documentation"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"accessing-model-service-apis",children:"Accessing Model Service APIs"}),"\n",(0,i.jsxs)(n.p,{children:["The Model Service exposes a list of RESTful APIs compatible with ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/introduction",children:"OpenAI's API"})," at the ",(0,i.jsx)(n.code,{children:"/v1"})," path. You can get the model API URL by clicking the ",(0,i.jsx)(n.strong,{children:"Copy"})," button of the selected model."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"modelservice-api-url",src:s(6930).A+"",width:"3010",height:"682"})}),"\n",(0,i.jsx)(n.h3,{id:"api-endpoints",children:"API Endpoints"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Route Path"}),(0,i.jsx)(n.th,{children:"Methods"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/chat/completions"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Perform chat completions using the model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/completions"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Perform standard completions using the model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/embeddings"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Generate embeddings using the model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/v1/models"})}),(0,i.jsx)(n.td,{children:"GET"}),(0,i.jsx)(n.td,{children:"List all available models."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/health"})}),(0,i.jsx)(n.td,{children:"GET"}),(0,i.jsx)(n.td,{children:"Check the health of the model service HTTP server."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/tokenize"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.a,{href:"https://platform.openai.com/tokenizer",children:"Tokenize"})," text using the running model service."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/detokenize"})}),(0,i.jsx)(n.td,{children:"POST"}),(0,i.jsx)(n.td,{children:"Detokenize tokens using the running model service."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/openapi.json"})}),(0,i.jsx)(n.td,{children:"GET, HEAD"}),(0,i.jsx)(n.td,{children:"Get the OpenAPI JSON specification for the model service."})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"api-usage-examples",children:"API Usage Examples"}),"\n",(0,i.jsx)(n.h4,{id:"curl-example",children:"cURL Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export LLMOS_API_KEY=myapikey\nexport API_BASE=192.168.31.100:8443/api/v1/namespaces/default/services/modelservice-qwen2:http/proxy/v1\ncurl -k -X POST \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $LLMOS_API_KEY" \\\n  -d \'{\n    "model": "Qwen/Qwen2.5-0.5B-Instruct",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful assistant."\n      },\n      {\n        "role": "user",\n        "content": "Say this is a test"\n      }\n    ],\n    "temperature": 0.9\n  }\' \\\n  $API_BASE/chat/completions\n'})}),"\n",(0,i.jsx)(n.p,{children:"Response Example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id":"chat-efffa70236bd4edda7e5420349339d45",\n  "object":"chat.completion",\n  "created":1727267645,\n  "model":"Qwen/Qwen2.5-0.5B-Instruct",\n  "choices":[\n    {\n      "index":0,\n      "message":{\n        "role":"assistant",\n        "content":"Yes, it is a test."\n      },\n      "logprobs":null,\n      "finish_reason":"stop"\n    }\n  ],\n  "usage":{\n    "prompt_tokens":24,\n    "total_tokens":32,\n    "completion_tokens":8\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"python-example",children:"Python Example"}),"\n",(0,i.jsx)(n.p,{children:"Since the API is compatible with OpenAI, you can use it as a drop-in replacement for OpenAI-based applications."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport httpx\n\n# Set up API key and base URL\nopenai_api_key = "llmos-5frck:xxxxxxxxxg79c9p5"\nopenai_api_base = "https://192.168.31.100:8443/api/v1/namespaces/default/services/modelservice-qwen2:http/proxy/v1"\nclient = OpenAI(\n   api_key=openai_api_key,\n   base_url=openai_api_base,\n   http_client=httpx.Client(verify=False), # Disable SSL verification or use a custom CA bundle.\n)\n\ncompletion = client.chat.completions.create(\n   model="Qwen/Qwen2.5-0.5B-Instruct",\n   messages=[{"role": "user", "content": "How do I output all files in a directory using Python?"}]\n)\nprint(completion.choices[0].message.content)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"notebooks-interaction",children:"Notebooks Interaction"}),"\n",(0,i.jsxs)(n.p,{children:["You can also interact with model services using the ",(0,i.jsx)(n.a,{href:"/docs/user_guide/llm_management/notebooks",children:"Notebooks"}),", which allows you to explore the model\u2019s capabilities more interactively using HTML, graphs, and more (e.g., using a Jupyter Notebook as below)."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"model-service-notebook",src:s(1378).A+"",width:"2870",height:"1752"})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.p,{children:"Within your LLMOS cluster, you can connect to the model service using its internal DNS name."}),(0,i.jsxs)(n.p,{children:["To get the internal DNS name, Click the ",(0,i.jsx)(n.strong,{children:"Internal URL"})," button of the model service."]})]}),"\n",(0,i.jsx)(n.h2,{id:"adding-a-huggingface-token",children:"Adding a HuggingFace Token"}),"\n",(0,i.jsxs)(n.p,{children:["If the model is authorized for downloading, you will need to add a ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"HuggingFace token"})," when creating the model service. To add a new HuggingFace token:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to the ",(0,i.jsx)(n.strong,{children:"Advanced > Secrets"})," page and click the ",(0,i.jsx)(n.strong,{children:"Create"})," button"]}),"\n",(0,i.jsxs)(n.li,{children:["Select the ",(0,i.jsx)(n.strong,{children:"Opaque"})," type.\n",(0,i.jsx)(n.img,{alt:"secret-create-opaque",src:s(5093).A+"",width:"3174",height:"916"})]}),"\n",(0,i.jsxs)(n.li,{children:["Select the ",(0,i.jsx)(n.strong,{children:"Namespace"})," and specify a meaningful ",(0,i.jsx)(n.strong,{children:"Name"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Specify the ",(0,i.jsx)(n.strong,{children:"key"})," e.g, ",(0,i.jsx)(n.code,{children:"token"}),", and the ",(0,i.jsx)(n.strong,{children:"value"})," as your HuggingFace token.\n",(0,i.jsx)(n.img,{alt:"secret-create-hf-token",src:s(265).A+"",width:"3118",height:"1066"})]}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:"Create"})," to save the secret. And you should see selected secret when creating the model service within the same namespace."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},6930:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-api-url-c05048b752752d5f8432e282fcb7e450.png"},1607:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-create-general-02b653655128735151d75a308889c494.png"},1072:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-create-resources-3b31a587bec4ba6c8cdf50b2be9fb121.png"},5782:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-create-volumes-5a9e1266bbb802eec371fb177bc05654.png"},7151:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-list-ec5f31a597a5b3d4873361c53a866709.png"},861:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-node-scheduling-7d730f1a68e0add35b1a80b4466810e8.png"},1378:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/modelservice-notebook-example-e42be55aac23840dfcf65231ddeddc17.png"},265:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/secret-create-hf-token-086bffab7e79dee78f7105417d22e4ce.png"},5093:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/secret-types-opaque-bb0c117ee345442ad345e26636970762.png"},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);